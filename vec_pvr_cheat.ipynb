{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import networkx as nx\n",
    "import dgl\n",
    "from dgl.nn import GATConv\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = 10000\n",
    "test_num = 1000\n",
    "length = 6\n",
    "tot_len = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_dataset(dataset):\n",
    "    n = len(dataset)\n",
    "    split_data = torch.zeros( (n,tot_len), dtype = torch.long )\n",
    "    for ii in range(n):\n",
    "        for i in range(tot_len):\n",
    "            split_data[ii,i] = torch.sum(dataset[ii,i:i+length])\n",
    "    return split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPVRDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, num, length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X = torch.randint(0,9,(num, tot_len) )\n",
    "        self.y = []\n",
    "        labels = torch.ones(length)\n",
    "        for ii in range(num):\n",
    "            y = torch.sum(self.X[ii,self.X[ii][0]:self.X[ii][0]+length]) \n",
    "            self.y.append(y % 10)\n",
    "            \n",
    "        self.z = proc_dataset(self.X).unsqueeze(dim = 1)\n",
    "            \n",
    "        self.G = dgl.from_networkx(nx.complete_graph(tot_len))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.z[idx], self.G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    X,y,z,graphs = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return torch.cat(X), torch.tensor(y, dtype = torch.long), torch.cat(z), batched_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = VPVRDataset(train_num, length)\n",
    "testset = VPVRDataset(test_num, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[16, 20, 26, 22, 19, 21, 28, 30, 30, 32, 27, 23, 15,  9,  3]])\n"
     ]
    }
   ],
   "source": [
    "print(trainset[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True,num_workers=1,collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=False,num_workers=1,collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.emb1 = nn.Embedding(10, 32)\n",
    "        self.x2 = torch.autograd.Variable(torch.randn([tot_len,32]),requires_grad=True)\n",
    "        \n",
    "        self.lin1 = nn.Linear(64,64)\n",
    "        self.gat = GATConv(64,64,1)\n",
    "        self.gat2 = GATConv(64,64,1)\n",
    "        \n",
    "        self.sum_pool = dgl.nn.SumPooling()\n",
    "        \n",
    "        self.lin2 = nn.Linear(128,32)\n",
    "        self.lin3 = nn.Linear(32,1)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        batch_size = x.shape[0]//tot_len\n",
    "        \n",
    "        x1 = self.emb1(x)\n",
    "        x2 = self.x2.repeat((batch_size, 1))\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim = 1)\n",
    "        x = self.lin1(x)\n",
    "        \n",
    "        x1 = self.gat(g, x).squeeze()\n",
    "        x2 = self.gat2(g, x).squeeze()\n",
    "        x = torch.cat([x1, x2], dim = 1)\n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = F.relu(x)\n",
    "        mask = self.lin3(x)\n",
    "        mask = mask.reshape(batch_size, tot_len)\n",
    "        mask = F.softmax(mask, dim = 1).unsqueeze(-1)\n",
    "        \n",
    "        return mask\n",
    "\n",
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.emb = nn.Embedding(60, 64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64,128)\n",
    "        self.fc2 = nn.Linear(128,10)\n",
    "\n",
    "    def forward(self, z, mask):\n",
    "        #x_emb = F.one_hot(z, num_classes=10)\n",
    "        x_emb = self.emb(z)\n",
    "        x = x_emb * mask\n",
    "        \n",
    "        x = torch.sum(x, 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        output = F.log_softmax(x, dim = 1)\n",
    "        return output\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(gen, dis, train_loader, optimizer, epoch):\n",
    "    gen.train()\n",
    "    dis.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    \n",
    "    n = len(train_loader.dataset)\n",
    "    for batch_idx, (data, target, z, graph) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        mask = gen(graph, data)\n",
    "        output = dis(z, mask)\n",
    "        \n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    print('Loss/train:', running_loss/n, 'Accuracy/train:', correct/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(gen, dis, test_loader, epoch):\n",
    "    gen.eval()\n",
    "    dis.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    n = len(test_loader.dataset)\n",
    "    with torch.no_grad():\n",
    "        for data, target, z, graph in test_loader:\n",
    "            mask = gen(graph, data)\n",
    "            output = dis(z, mask)\n",
    "        \n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "    print('Loss/test:', test_loss/n, 'Accuracy/test:', correct/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/train: 0.029911385667324066 Accuracy/train: 0.3278\n",
      "Loss/test: 1.407561149597168 Accuracy/test: 0.509\n",
      "Loss/train: 0.01793164694905281 Accuracy/train: 0.5747\n",
      "Loss/test: 0.9192896652221679 Accuracy/test: 0.655\n",
      "Loss/train: 0.013748155176639557 Accuracy/train: 0.6613\n",
      "Loss/test: 0.7940436477661132 Accuracy/test: 0.672\n",
      "Loss/train: 0.012653006130456924 Accuracy/train: 0.6857\n",
      "Loss/test: 0.7684544048309326 Accuracy/test: 0.684\n",
      "Loss/train: 0.011475351110100746 Accuracy/train: 0.7103\n",
      "Loss/test: 0.7421910438537598 Accuracy/test: 0.709\n",
      "Loss/train: 0.01043642218708992 Accuracy/train: 0.7367\n",
      "Loss/test: 0.799012071609497 Accuracy/test: 0.679\n",
      "Loss/train: 0.011169840887188911 Accuracy/train: 0.7091\n",
      "Loss/test: 0.7363619060516358 Accuracy/test: 0.692\n",
      "Loss/train: 0.009783775967359543 Accuracy/train: 0.7444\n",
      "Loss/test: 0.6332630386352539 Accuracy/test: 0.729\n",
      "Loss/train: 0.010664560198783875 Accuracy/train: 0.7391\n",
      "Loss/test: 0.7300996646881104 Accuracy/test: 0.706\n",
      "Loss/train: 0.01134096976518631 Accuracy/train: 0.7171\n",
      "Loss/test: 0.7780330982208252 Accuracy/test: 0.681\n",
      "Loss/train: 0.009236012861132622 Accuracy/train: 0.7534\n",
      "Loss/test: 0.5359466648101807 Accuracy/test: 0.757\n",
      "Loss/train: 0.008179464431107045 Accuracy/train: 0.7764\n",
      "Loss/test: 0.5246283321380615 Accuracy/test: 0.783\n",
      "Loss/train: 0.008015538208186627 Accuracy/train: 0.7966\n",
      "Loss/test: 0.8762091484069824 Accuracy/test: 0.696\n",
      "Loss/train: 0.008776591400802135 Accuracy/train: 0.7777\n",
      "Loss/test: 0.3690070810317993 Accuracy/test: 0.849\n",
      "Loss/train: 0.004724711867421866 Accuracy/train: 0.8914\n",
      "Loss/test: 1.0961843795776367 Accuracy/test: 0.662\n",
      "Loss/train: 0.008600993484258652 Accuracy/train: 0.7649\n",
      "Loss/test: 0.37033441066741946 Accuracy/test: 0.842\n",
      "Loss/train: 0.00539156534820795 Accuracy/train: 0.8446\n",
      "Loss/test: 0.32456775665283205 Accuracy/test: 0.86\n",
      "Loss/train: 0.006737929767370224 Accuracy/train: 0.8787\n",
      "Loss/test: 0.09691469192504883 Accuracy/test: 0.976\n",
      "Loss/train: 0.00023482645738404244 Accuracy/train: 0.9986\n",
      "Loss/test: 0.008707033053040504 Accuracy/test: 0.999\n",
      "Loss/train: 5.460922602796927e-05 Accuracy/train: 1.0\n",
      "Loss/test: 0.007136692807078362 Accuracy/test: 0.999\n",
      "Loss/train: 3.456153782317415e-05 Accuracy/train: 1.0\n",
      "Loss/test: 0.006371792532503605 Accuracy/test: 0.999\n",
      "Loss/train: 2.4601380643434822e-05 Accuracy/train: 1.0\n",
      "Loss/test: 0.006007580615580082 Accuracy/test: 0.999\n",
      "Loss/train: 1.8875425931764767e-05 Accuracy/train: 1.0\n",
      "Loss/test: 0.0057813257630914446 Accuracy/test: 0.999\n",
      "Loss/train: 1.4950657504959963e-05 Accuracy/train: 1.0\n",
      "Loss/test: 0.005607083573937416 Accuracy/test: 0.999\n",
      "Loss/train: 1.2084779067663476e-05 Accuracy/train: 1.0\n",
      "Loss/test: 0.005502996383234858 Accuracy/test: 0.999\n",
      "Loss/train: 9.981288318522275e-06 Accuracy/train: 1.0\n",
      "Loss/test: 0.0054204903803765775 Accuracy/test: 0.999\n",
      "Loss/train: 8.571742876665666e-06 Accuracy/train: 1.0\n",
      "Loss/test: 0.005372920820489526 Accuracy/test: 0.999\n",
      "Loss/train: 7.231516030151397e-06 Accuracy/train: 1.0\n",
      "Loss/test: 0.0053304352117702365 Accuracy/test: 0.999\n",
      "Loss/train: 6.171693104261067e-06 Accuracy/train: 1.0\n",
      "Loss/test: 0.005302704932168126 Accuracy/test: 0.999\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(list(generator.parameters())+list(discriminator.parameters()), lr=1e-3)\n",
    "for epoch in range(1, 30):\n",
    "    train(generator, discriminator, train_loader, optimizer, epoch)\n",
    "    test(generator, discriminator, test_loader, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
